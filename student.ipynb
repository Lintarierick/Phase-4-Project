{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpQH9kga-YzV"
   },
   "source": [
    "# **Unveiling Film Tastes: A Personalized Movie Journey through Smart Recommendations.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqAmySTm_S7C"
   },
   "source": [
    "Group 2:\n",
    "\n",
    "         Eric Lintari.\n",
    "        Kamande Karigi.\n",
    "        Lucy Waruguru.\n",
    "        Victor Gachie.\n",
    "        Cynthia Wanyeki.\n",
    "\n",
    "Technical Mentor:  Diana Mong'ina.\n",
    "\n",
    "Phase 4 Project: Recommendation System.\n",
    "\n",
    "Deadline: 18th January 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sihah2ahAOMR"
   },
   "source": [
    "# **TABLE OF CONTENTS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPHJBbT_0mA0"
   },
   "source": [
    "\n",
    "![mrec1.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiX6Ad9pA3Yp"
   },
   "source": [
    "# **INTRODUCTION.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa_2H7auA7iC"
   },
   "source": [
    "The goal of this project is to delve into user behavior within the rise of movie streaming platforms and construct a model that enhances user engagement, ultimately leading to an enriched cinematic experience.\n",
    "\n",
    "In today's digital age, online streaming has become the predominant avenue for accessing diverse cinematic content. The vastness of movie catalogs poses a challenge for users in selecting content tailored to their individual preferences. By comprehensively analyzing user behavior, we aim to build a recommendation system that simplifies the process of navigating through this cinematic maze, ensuring that users discover and enjoy movies that align closely with their tastes.\n",
    "\n",
    "The significance of such a system lies in its potential to not only improve user satisfaction and retention but also to foster a dynamic and engaging movie-watching environment. Leveraging real-time project datasets sourced from platforms like [movielens](https://grouplens.org/datasets/movielens/latest/), our analysis will focus on a variety of user interactions, ratings, and viewing patterns to build a model that resonates with the diverse preferences of our audience. Through this project, we aspire to bring forth a personalized and enjoyable cinematic journey for each user.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9PIKFsd1u2S"
   },
   "source": [
    "# **BUSINESS UNDERSTANDING.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PNcpWMi203j"
   },
   "source": [
    "The film industry boasts of a vast array of genres, ranging from action and drama to comedy, science fiction, horror, and beyond. Each genre caters to distinct tastes and moods, offering a rich tapestry of cinematic experiences. While this diversity is an advantage for movie enthusiasts, it also poses a challenge—the dilemma of choosing the perfect film from the extensive selection.\n",
    "\n",
    "For users, navigating through this abundance of options can be a daunting task. The sheer volume of movies available makes decision-making overwhelming, especially when the goal is to ensure not just any watch but an enjoyable and satisfying cinematic experience. Consequently, businesses operating in this industry must delve into the complexities of user preferences to provide tailored recommendations and enhance the overall movie-watching experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xTCUhuw3Yjr"
   },
   "source": [
    "# **PROBLEM STATEMENT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIPsvsoQ4ZVa"
   },
   "source": [
    "A streaming platform's success depends on how well it keeps users happy and interested. One big factor is the recommendation system – the feature that suggests movies for you to watch. This is crucial because there are so many movies on the platform, and everyone likes different things. It's like trying to find your way in a big, confusing maze.\n",
    "\n",
    "People's taste in movies is very personal. What one person loves, another might not enjoy at all. So, predicting what movies a person will like is tricky. That's where our project comes in. We want to create a recommendation system that looks at what movies you've watched before, how you've rated them, and what other people who like similar things enjoyed. This way, we can suggest movies that match your taste, making it easier for you to discover and enjoy films that you'll probably love.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVmo4bsC4b8z"
   },
   "source": [
    "# **OBJECTIVES.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRmq_wFP46IC"
   },
   "source": [
    "\n",
    "1.\t**Enhance User Experience:**\n",
    "•\tIncrease user engagement by providing personalized movie recommendations.\n",
    "•\tImprove user satisfaction by suggesting movies aligned with individual tastes.\n",
    "\n",
    "\n",
    "2.\t**Increase User Retention:**\n",
    "•\tRetain users by offering a compelling and personalized movie-watching experience.\n",
    "•\tEncourage users to spend more time on the platform through relevant recommendations.\n",
    "\n",
    "\n",
    "3.\t**Drive Revenue:**\n",
    "•\tIncrease the consumption of movies by recommending content that aligns with users' interests.\n",
    "•\tPotentially increase revenue through ad views or premium subscriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-VRci5w5K2R"
   },
   "source": [
    "# **SOURCE OF DATA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq6Y9l5959d7"
   },
   "source": [
    "\n",
    "The data used was sourced from [MovieLens](https://grouplens.org/datasets/movielens/latest/). We used the small dataset due to limited computational power. We selected the four files in line with project objectives.\n",
    "\n",
    "\n",
    " **Movies.csv:** Contains information about the movies.\n",
    "\n",
    "\n",
    "movieId - Unique identifier for each movie.\n",
    "\n",
    "title - The movie titles.\n",
    "\n",
    "genre - The various genres a movie falls into.\n",
    "\n",
    "**Links.csv:**\n",
    "\n",
    "•\tColumns: movieId, imdbId, tmdbId\n",
    "\n",
    "•\tInsights:\n",
    "•\tmovieId is likely the key that can be used to link this DataFrame with other DataFrames.\n",
    "•\timdbId and tmdbId could be useful for external references to IMDb and TMDB.\n",
    "\n",
    "\n",
    "**Ratings.csv:**\n",
    "\n",
    "•\tColumns: userId, movieId, rating, timestamp\n",
    "\n",
    "•\tInsights:\n",
    "•\tuserId and movieId are crucial for identifying users and movies.\n",
    "\n",
    "•\trating represents the user's rating for a movie.\n",
    "\n",
    "•\ttimestamp indicates when the rating was given.\n",
    "\n",
    "\n",
    "**Tags.csv:**\n",
    "\n",
    "•\tColumns: userId, movieId, tag, timestamp\n",
    "\n",
    "•\tInsights:\n",
    "•\tSimilar structure to the Ratings DataFrame.\n",
    "\n",
    "•\tAdditional information in the form of tags, providing more context about movies.\n",
    "\n",
    "•\tThe tag column is of type object (likely string), which contains textual information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv6hG5Uo8vaR"
   },
   "source": [
    "# **IMPORTING LIBRARIES.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeVwAl5M9HlA",
    "outputId": "dc2196df-7447-407e-a5c5-4c1dfa0ead47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ek\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ek\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ek\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\EK\\AppData\\Local\\Temp\\pip-install-75squ6pv\\scikit-surprise_f2b6969e903c44c5b4774b84b4946736\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\EK\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-surprise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_validate\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "!pip install scikit-surprise\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import KNNBasic, KNNWithMeans\n",
    "from surprise import SVD\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9k-1PEu-V54"
   },
   "source": [
    "**Loading Datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dX4Eus1wC-xZ",
    "outputId": "e29df5b9-72be-47b5-bd19-b46fcd2d0604"
   },
   "outputs": [],
   "source": [
    "#Loading and reading the datasets.\n",
    "\n",
    "# List of CSV files along with corresponding column names\n",
    "csv_files_and_columns = [\n",
    "    ('movies.csv', ['movieId', 'title', 'genres']),\n",
    "    ('links.csv', ['movieId', 'imdbId', 'tmdbId']),\n",
    "    ('tags.csv', ['userId', 'movieId', 'tag', 'timestamp']),\n",
    "    ('ratings.csv', ['userId', 'movieId', 'rating', 'timestamp'])\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each CSV file and its corresponding columns\n",
    "for csv_file, columns in csv_files_and_columns:\n",
    "    # Extract the DataFrame name from the file name (remove extension)\n",
    "    df_name = csv_file.split('.')[0]\n",
    "\n",
    "    # Read the CSV file into a DataFrame with specified column names\n",
    "    dataframes[df_name] = pd.read_csv(csv_file, delimiter=',', header=0, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Access the DataFrames using keys\n",
    "movies_df = dataframes['movies']\n",
    "links_df = dataframes['links']\n",
    "tags_df = dataframes['tags']\n",
    "ratings_df = dataframes['ratings']\n",
    "\n",
    "# Print the head of each DataFrame\n",
    "for df_name, df in dataframes.items():\n",
    "    print(f\"\\n{df_name}_df:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFMEhr16DimM"
   },
   "source": [
    "All Datasets have a common column, movieID. This will be used in the merging of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZbMiPmEE3vK"
   },
   "source": [
    "# **Data Understanding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3uKZxMKDubh",
    "outputId": "09a3feab-eb10-4dd7-a07f-4e9642172d88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display information for each dataset\n",
    "print(\"Movies DataFrame:\")\n",
    "print(\"Shape:\", movies_df.shape)\n",
    "print(movies_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ratings DataFrame:\")\n",
    "print(\"Shape:\", ratings_df.shape)\n",
    "print(ratings_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Tags DataFrame:\")\n",
    "print(\"Shape:\", tags_df.shape)\n",
    "print(tags_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Links DataFrame:\")\n",
    "print(\"Shape:\", links_df.shape)\n",
    "print(links_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7pxL-kCFHCL"
   },
   "source": [
    "**Movies Dataframe:**\n",
    "\n",
    "Rows-9742\n",
    "\n",
    "Columns-3\n",
    "\n",
    "\n",
    "**Ratings Dataframe:**\n",
    "\n",
    "Rows-100,836\n",
    "\n",
    "Columns-4\n",
    "\n",
    "\n",
    "**Links Dataframe:**\n",
    "\n",
    "Rows-9742\n",
    "\n",
    "Columns-3\n",
    "\n",
    "\n",
    "**Tags Dataframe:**\n",
    "\n",
    "Rows-3683\n",
    "\n",
    "Columns-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962OpB0uGFiM"
   },
   "source": [
    "Check for Datatypes of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hrhnx6QFIxsZ",
    "outputId": "825abc7d-ebcb-46cc-84f5-fa5138f83eb0"
   },
   "outputs": [],
   "source": [
    "# Function to count numerical and categorical variables\n",
    "def count_variable_types(df):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    return len(numerical_cols), len(categorical_cols)\n",
    "\n",
    "# Display counts for each dataset\n",
    "movies_numerical, movies_categorical = count_variable_types(movies_df)\n",
    "links_numerical, links_categorical = count_variable_types(links_df)\n",
    "ratings_numerical, ratings_categorical = count_variable_types(ratings_df)\n",
    "tags_numerical, tags_categorical = count_variable_types(tags_df)\n",
    "\n",
    "# Print results\n",
    "print(f\"Movies Dataset - Numerical Variables: {movies_numerical}, Categorical Variables: {movies_categorical}\")\n",
    "print(f\"Links Dataset - Numerical Variables: {links_numerical}, Categorical Variables: {links_categorical}\")\n",
    "print(f\"Ratings Dataset - Numerical Variables: {ratings_numerical}, Categorical Variables: {ratings_categorical}\")\n",
    "print(f\"Tags Dataset - Numerical Variables: {tags_numerical}, Categorical Variables: {tags_categorical}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l_hCaw4JU8q"
   },
   "source": [
    "**Movies Dataset:**\n",
    "\n",
    "Numerical Variables: 1 (movieId)\n",
    "\n",
    "Categorical Variables: 2 (title and genres)\n",
    "\n",
    "\n",
    "**Links Dataset:**\n",
    "\n",
    "Numerical Variables: 3 (movieId, imdbId, tmdbId)\n",
    "\n",
    "Categorical Variables: 0\n",
    "\n",
    "\n",
    "\n",
    "**Ratings Dataset:**\n",
    "\n",
    "Numerical Variables: 4 (userId, movieId, rating, timestamp)\n",
    "\n",
    "Categorical Variables: 0\n",
    "\n",
    "\n",
    "**Tags Dataset:**\n",
    "\n",
    "Numerical Variables: 3 (userId, movieId, timestamp)\n",
    "\n",
    "Categorical Variables: 1 (tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOp4hAB0KU9B"
   },
   "source": [
    "Merging Datsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqVOdCbsKDk-",
    "outputId": "7e853851-dc3c-4d5e-8eaa-99b1e5c04833"
   },
   "outputs": [],
   "source": [
    "# Merge movies_df with links_df\n",
    "merged_df = pd.merge(movies_df, links_df, on='movieId', how='left') #to combine general movie information with additional details.\n",
    "\n",
    "# Merge the result with tags_df\n",
    "merged_df = pd.merge(merged_df, tags_df, on='movieId', how='left')  #to incorporate tags.\n",
    "\n",
    "# Merge the final result with ratings_df\n",
    "final_merged_df = pd.merge(merged_df, ratings_df, on='movieId', how='left')  #to include user ratings.\n",
    "\n",
    "# Create a new DataFrame by copying the final merged DataFrame\n",
    "new_df = final_merged_df.copy()\n",
    "\n",
    "# Display information, data types, and the first few rows of the new DataFrame\n",
    "print(\"Info for new DataFrame:\")\n",
    "print(new_df.info())\n",
    "\n",
    "# Display data types of each column\n",
    "print(\"\\nData Types of Columns:\")\n",
    "print(new_df.dtypes)\n",
    "\n",
    "# Display the first few rows of the new DataFrame\n",
    "print(\"\\nFirst few rows of new DataFrame:\")\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-pov7fkOC6F"
   },
   "source": [
    "The merged dataframe has:\n",
    "\n",
    "Rows-285,783.\n",
    "\n",
    "Columns-11.\n",
    "\n",
    "Numerical Variables-8. (movieID, imdbID, tmdbID, UserID_X, timestamp_x, UserID_y, timestamp_y, rating.)\n",
    "\n",
    "Categorical Variables-3. (title, genre, tag.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzRkjNiCQtNP"
   },
   "source": [
    "Renaming the Columns with similar names to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8sLJD01QLQS",
    "outputId": "693a48f4-8ad1-4fd6-f0c2-d66fc1b489f8"
   },
   "outputs": [],
   "source": [
    "# Rename the first set of columns from tags dataframe.\n",
    "new_df = new_df.rename(columns={'userId_x': 'tags_userId', 'timestamp_x': 'tags_timestamp'})\n",
    "\n",
    "# Rename the second set of columns from ratings dataframe.\n",
    "new_df = new_df.rename(columns={'userId_y': 'ratings_userId', 'timestamp_y': 'ratings_timestamp'})\n",
    "new_df.rename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ipyMal3SKuq",
    "outputId": "acd70dd7-9a22-4aa4-e394-fe7838a04a14"
   },
   "outputs": [],
   "source": [
    "# info of the new dataframe.\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Wj28gFuBTW9I",
    "outputId": "b55d45f9-c3b2-491b-92f0-3fe74ee42f92"
   },
   "outputs": [],
   "source": [
    "# summary statistics for the new dataframe.\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M38nSDZvTtE0"
   },
   "source": [
    "The dataset seems to cover a diverse range of movies, users, and timestamps.\n",
    "\n",
    "Ratings exhibit a moderate spread, with a mean around 3.84, indicating that, on average, movies are rated relatively positively.\n",
    "\n",
    "The temporal distribution of tags and ratings spans a significant period, as indicated by the timestamp statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIaUC9sjQ6Dk"
   },
   "source": [
    "# **DATA CLEANING.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rDshHHLWnks"
   },
   "source": [
    "Check missing values, outliers, duplicates, inconsistencies, memory optimization, and correct data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2O2Yvx3Qd1v",
    "outputId": "3f2f621a-6d33-4ed8-bf63-d07a293e61b9"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = new_df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Check for outliers (assuming numeric columns)\n",
    "numeric_columns = new_df.select_dtypes(include=[np.number]).columns\n",
    "outliers = new_df[numeric_columns].apply(lambda x: (x - x.mean()).abs() > 3 * x.std())\n",
    "print(\"\\nOutliers:\\n\", outliers.sum())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = new_df.duplicated().sum()\n",
    "print(\"\\nDuplicates:\", duplicates)\n",
    "\n",
    "# Check for inconsistencies or errors (in genres)\n",
    "# Assuming genres column has pipe-separated values\n",
    "inconsistent_genres = new_df['genres'].apply(lambda x: '|' in x)\n",
    "print(\"\\nInconsistent Genres:\", inconsistent_genres.sum())\n",
    "\n",
    "# Optimize memory usage\n",
    "new_df_optimized = new_df.copy()\n",
    "for col in new_df_optimized.columns:\n",
    "    if new_df_optimized[col].dtype == 'object':\n",
    "        new_df_optimized[col] = new_df_optimized[col].astype('category')\n",
    "\n",
    "# Display information after optimization\n",
    "print(\"\\nOptimized DataFrame:\")\n",
    "print(\"Shape:\", new_df_optimized.shape)\n",
    "print(new_df_optimized.info())\n",
    "\n",
    "# Ensure correct data types for each column\n",
    "# Convert timestamp columns to datetime\n",
    "timestamp_columns = ['tags_timestamp', 'ratings_timestamp']\n",
    "new_df_optimized[timestamp_columns] = new_df_optimized[timestamp_columns].apply(pd.to_datetime, unit='s')\n",
    "\n",
    "# Display information after ensuring correct data types\n",
    "print(\"\\nDataFrame with Correct Data Types:\")\n",
    "print(new_df_optimized.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1agaU49YZRlY"
   },
   "source": [
    "**Missing Values:**\n",
    "\n",
    "Missing values were identified in several columns (tmdbId, tags_userId, tag, tags_timestamp, ratings_userId, rating, ratings_timestamp).\n",
    "Imputation strategies were suggested, such as filling numeric columns with mean values, categorical columns with a placeholder, and timestamp columns with a specific date.\n",
    "\n",
    "\n",
    "**Outliers:**\n",
    "\n",
    "Outliers were detected in various numeric columns (movieId, imdbId, tmdbId, tags_userId, rating).\n",
    "Further investigation and treatment of outliers were recommended based on the nature of the data.\n",
    "\n",
    "\n",
    "**Duplicates:**\n",
    "\n",
    "No duplicates were found in the dataset.\n",
    "\n",
    "\n",
    "**Inconsistent Genres:**\n",
    "\n",
    "A large number of entries were identified as having inconsistent genres.\n",
    "A manual examination and cleaning strategy were suggested to standardize genre entries.\n",
    "\n",
    "\n",
    "**Memory Optimization:**\n",
    "\n",
    "Memory optimization was performed by converting object-type columns to the category, resulting in a reduced memory footprint.\n",
    "\n",
    "\n",
    "**Data Types:**\n",
    "\n",
    "Correct data types were ensured for each column, such as converting timestamp columns to datetime.\n",
    "\n",
    "\n",
    "The dataset has been preprocessed and is now ready for further analysis(EDA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2Rt_zzmZ58X"
   },
   "source": [
    "Handle  missing values, outliers, duplicates and inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqyEkGvBa7fB",
    "outputId": "d83462c5-02a2-465b-b575-a518390a4866"
   },
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "new_df = new_df.dropna()\n",
    "\n",
    "# Handle Outliers (for the 'rating' column)\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = zscore(new_df['rating'])\n",
    "outliers = (z_scores > 3) | (z_scores < -3)\n",
    "new_df = new_df[~outliers]\n",
    "\n",
    "# Handle Inconsistent Genres\n",
    "new_df['genres'] = new_df['genres'].str.split('|')\n",
    "unique_genres = set()\n",
    "for genres_list in new_df['genres']:\n",
    "    unique_genres.update(genres_list)\n",
    "\n",
    "# One-Hot Encoding for Genres\n",
    "for genre in unique_genres:\n",
    "    new_df[genre] = new_df['genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "# Display the information about the cleaned dataset\n",
    "print(\"Info for Cleaned Dataset:\")\n",
    "print(new_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SHUSS-0c7Dt"
   },
   "source": [
    "\n",
    "\n",
    " Rows: 227,584\n",
    "\n",
    "Columns: 31\n",
    "\n",
    "**Data Types:**\n",
    "\n",
    "Numerical : 8\n",
    "Categorical: 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The preprocessing steps have resulted in a cleaned dataset with no missing values, handled outliers in the rating column, and transformed the genres column into a more usable format with one-hot encoding for each genre. The dataset is ready for further analysis and the development of a recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCSf7Almg0wo"
   },
   "source": [
    "Dealing with the genre column.\n",
    "\n",
    "We decided to keep the first genre, with the assumption that the first genre listed is the most important genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2EPi5bHbxGy",
    "outputId": "f95784c1-17d7-4470-fa46-b90afe4952db"
   },
   "outputs": [],
   "source": [
    "# Keep only the desired columns\n",
    "selected_columns = ['movieId', 'title', 'genres', 'imdbId', 'tmdbId', 'tags_userId', 'tag', 'tags_timestamp', 'ratings_userId', 'rating', 'ratings_timestamp']\n",
    "cleaned_df = new_df[selected_columns].copy()\n",
    "\n",
    "# Handle Genres (Keep only the first genre)\n",
    "cleaned_df['genres'] = cleaned_df['genres'].astype(str).apply(lambda x: x.split('|')[0] if '|' in x else x)\n",
    "\n",
    "# Display information about the cleaned DataFrame\n",
    "print(\"Info for Cleaned Dataset:\")\n",
    "print(cleaned_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gMWjgOMhKfo"
   },
   "source": [
    "Rows- 227,584.\n",
    "\n",
    "Columns-11.\n",
    "\n",
    "\n",
    "The dataset has been cleaned, with missing values handled, outliers removed from the rating column, and the genres column simplified to only keep the first genre. The memory usage has also been reduced from the original dataset. The cleaning process aims to prepare the data for further analysis or model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBqkq4aCjxuX"
   },
   "source": [
    "Checking Placeholders.\n",
    "\n",
    "eg, NAN, NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4bgkXLwfK32"
   },
   "outputs": [],
   "source": [
    "# confirming the absence of placeholders in my dataframe.\n",
    "\n",
    "# Numeric placeholder values.\n",
    "placeholders_count = {}\n",
    "\n",
    "# Loop through each column in the dataset\n",
    "for column in cleaned_df.columns:\n",
    "    # Count occurrences of unique values in each column\n",
    "    value_counts = cleaned_df[column].value_counts()\n",
    "\n",
    "    # Check if there are any placeholder values (e.g., NaN, 'Unknown', NULL.)\n",
    "    # and store the count in the placeholders_count dictionary\n",
    "    if pd.NA in value_counts:\n",
    "        placeholders_count[column] = value_counts[pd.NA]\n",
    "\n",
    "# Display the count of placeholders for each column\n",
    "for column, count in placeholders_count.items():\n",
    "    print(f\"Column '{column}' has {count} placeholder values.\")\n",
    "\n",
    "\n",
    "\n",
    "# Non-numeric placeholder values.\n",
    "placeholders_count = {}\n",
    "\n",
    "# Loop through each column in the dataset\n",
    "for column in cleaned_df.columns:\n",
    "    # Check for non-numeric placeholder values (NaN)\n",
    "    count_nan = cleaned_df[column].isna().sum()\n",
    "\n",
    "    # Store the count in the placeholders_count dictionary\n",
    "    if count_nan > 0:\n",
    "        placeholders_count[column] = count_nan\n",
    "\n",
    "# Display the count of placeholders for each column\n",
    "for column, count in placeholders_count.items():\n",
    "    print(f\"Column '{column}' has {count} placeholder values.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9dWdYnVnqd3"
   },
   "source": [
    "The dataframe does not have placeholders and is well cleaned for EDA and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Uqg7hOYYmaEz",
    "outputId": "d557494b-0774-47f8-af4b-7b56038509fd"
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics for numeric columns\n",
    "cleaned_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtuXkoiOke_z"
   },
   "source": [
    "**Insights.**\n",
    "\n",
    "**Data Completeness:** The dataset appears to be relatively complete, with no missing values in the key columns relevant to building a recommendation system (movieId, userId, rating). This is essential for accurate model training.\n",
    "\n",
    "**User Engagement:** The dataset contains a diverse range of user interactions, including tags and ratings. This variety in user engagement signals the potential for building a recommendation system that considers multiple aspects of user preferences.\n",
    "\n",
    "**Timestamps:** Timestamps are available for both tags and ratings. This temporal information can be valuable for creating time-aware recommendation models, allowing the system to adapt to changing user preferences over time.\n",
    "\n",
    "**Rating Distribution:** The ratings column shows a distribution with a mean around 4.04, indicating that users generally provide positive ratings. Understanding the distribution helps in selecting appropriate algorithms for recommendation and setting relevant thresholds.\n",
    "\n",
    "**Genre Information:** The genres column provides information about the genres associated with each movie. This can be leveraged to create content-based recommendation features, enhancing the system's ability to understand user preferences based on genre affinity.\n",
    "\n",
    "**User IDs and Movie IDs:** The user and movie IDs are well-represented, indicating a diverse set of users and movies in the dataset. This diversity is crucial for training a recommendation system that can cater to various user tastes.\n",
    "\n",
    "**Consistency in Timestamps:** Timestamps for both tags and ratings are consistent, facilitating the alignment of different types of user interactions over time.\n",
    "\n",
    "\n",
    " The Dataset seems promising for building a recommendation system. Its completeness, diversity, and availability of relevant features make it a good starting point for further exploration and model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx3ROp2GkpQZ"
   },
   "source": [
    "# **EXPLORATORY DATA ANALYSIS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk86f79ipD_x"
   },
   "source": [
    "#### Distribution of Ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "kLqhm1mWjVnj",
    "outputId": "5975dbbe-f6de-486a-b6d2-11714e52a282"
   },
   "outputs": [],
   "source": [
    "# Distribution of ratings\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(cleaned_df['rating'], bins=5, kde=True)\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram provides a visual representation of the distribution of movie ratings, helping us understand the overall pattern and characteristics of ratings in our dataset.\n",
    "The histogram illustrates that the majority of movies in the dataset have positive ratings above 3.0, indicating a favorable overall sentiment. While there is a skew towards higher ratings, there are still movies with lower ratings, suggesting a diverse range of audience opinions. Overall, the dataset exhibits a positive trend in audience reception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DhiVs__qCfT"
   },
   "source": [
    "#### Distribution of Genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ceZi_rQYqQnV",
    "outputId": "e498ddfc-d4ec-479f-9fec-6fceea5f4a3c"
   },
   "outputs": [],
   "source": [
    "# Top 10 genres\n",
    "genres_counts = cleaned_df['genres'].value_counts().head(10)\n",
    "\n",
    "# Plotting the value counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(genres_counts.index, genres_counts.values, color='green')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 Value Counts for Genres')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FgPaUGYrqHz"
   },
   "source": [
    "The exploration of genre counts in the dataset highlights several noteworthy trends. Genres like Comedy/Crime/Drama/Thriller, Action/Crime/Drama/Thriller, and Action/Adventure/Sci-Fi emerge as top-performing categories, boasting higher counts and reflecting a significant presence and popularity among viewers. Conversely, genres such as Comedy/Romance, Drama/Romance/Sci-Fi, and Crime/Drama exhibit lower counts, indicating a comparatively lesser prevalence in the dataset or popularity among the audience. This analysis provides valuable insights into the distribution of genres, allowing for a better understanding of audience preferences and the relative popularity of different movie genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfpieEvushUy"
   },
   "source": [
    "#### Word Cloud for genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "_gZXz3J-uhOI",
    "outputId": "a6588c30-850e-4da3-8f34-768b9c67da7d"
   },
   "outputs": [],
   "source": [
    "# Extract the genres column\n",
    "genres_text = '|'.join(cleaned_df['genres'])\n",
    "genres_list = genres_text.split('|')\n",
    "\n",
    "# Create the word frequency dictionary\n",
    "word_frequency = {}\n",
    "for genre in genres_list:\n",
    "    word_frequency[genre] = word_frequency.get(genre, 0) + 1\n",
    "\n",
    "# Create the WordCloud object with adjusted figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "# Generate the word cloud from the word frequency\n",
    "wordcloud.generate_from_frequencies(word_frequency)\n",
    "\n",
    "# Display the word cloud with tight layout and no axis\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the genres column\n",
    "genres_text = '|'.join(cleaned_df['genres'])\n",
    "genres_list = genres_text.split('|')\n",
    "\n",
    "# Create the word frequency dictionary\n",
    "word_frequency = {}\n",
    "for genre in genres_list:\n",
    "    word_frequency[genre] = word_frequency.get(genre, 0) + 1\n",
    "\n",
    "# Create the WordCloud object with adjusted figure size and min_font_size\n",
    "plt.figure(figsize=(12, 8))\n",
    "wordcloud = WordCloud(background_color='white', min_font_size=8)\n",
    "\n",
    "# Generate the word cloud from the word frequency\n",
    "wordcloud.generate_from_frequencies(word_frequency)\n",
    "\n",
    "# Display the word cloud with tight layout and no axis\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger words represent genres that occur more frequently in the dataset while Smaller words indicates genres that are less common or have lower occurences. The most prevalent genres among the movies in the dataset are \"Comedy|Crime|Drama|Thriller\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nuNtj-kveoD"
   },
   "source": [
    "#### Movies Distribution by Year Released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "zn34oWhtsVM3",
    "outputId": "efd6be1b-2bbb-4a99-b3e1-d72ea1c34d0d"
   },
   "outputs": [],
   "source": [
    "# Extract the year and create a new \"year released\" column\n",
    "cleaned_df['year released'] = cleaned_df['title'].str.extract(r'\\((\\d{4})\\)')\n",
    "\n",
    "year_released_counts = cleaned_df['year released'].value_counts().head(10)\n",
    "\n",
    "# Sort the value counts by index (year) in descending order\n",
    "year_released_counts = year_released_counts\n",
    "#print(year_released_counts)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(year_released_counts.index.astype(str), year_released_counts.values, color='darkblue')\n",
    "plt.xlabel('Year Released')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 Counts for Year Released')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubIdHPl8xtiD"
   },
   "source": [
    "The bar chart illustrates the distribution of movie releases across different years based on the dataset. The chart provides a clear overview of the top 10 years with the most significant presence in the dataset, offering insights into the temporal distribution of movies and emphasizing certain periods of heightened cinematic activity. The most prominent years are 1994 with the highest number of movie releases, contributing 66, 749 releases, 1999 and 1995 with 19, 241 and 11, 095 releases, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "5AVupnY0xxnU",
    "outputId": "7f6cfecf-27b6-4f6f-d724-5ec52b6644d7"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing 'year released' values\n",
    "cleaned_df.dropna(subset=['year released'], inplace=True)\n",
    "\n",
    "# Convert the 'year released' column to integers\n",
    "cleaned_df['year released'] = cleaned_df['year released'].astype(int)\n",
    "\n",
    "# Function to compute decades from year released\n",
    "def year_to_decade(year):\n",
    "    return str(year // 10 * 10) + 's'\n",
    "\n",
    "# Apply the function to create the 'decade' column\n",
    "cleaned_df['decade'] = cleaned_df['year released'].apply(year_to_decade)\n",
    "decade_counts = cleaned_df['decade'].value_counts()\n",
    "\n",
    "# Sort the value counts in descending order\n",
    "decade_counts = decade_counts.sort_values(ascending=False)\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=decade_counts.index, y=decade_counts.values, color='blue')\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Movie Counts by Decade')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSBae-gkyxJs"
   },
   "source": [
    "The visualization provides insights into the distribution of movies over decades, allowing us to observe trends in movie production over time.Most movies were released in the 1990s followed by 2000s while the least were released in 1910s.We also observe a steady increase of movie production from 1920s which peaked in the 1990s before declining in the new millenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNFY7lfMzDqe"
   },
   "source": [
    "#### Distribution of Ratings by genre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "xNU0d8RzyaPL",
    "outputId": "e37426e0-e97f-4993-a416-0dc608667b41"
   },
   "outputs": [],
   "source": [
    "# Distribution of ratings by top five and bottom five rated movies\n",
    "#  mean rating for each genre\n",
    "genre_ratings = cleaned_df.groupby('genres')['rating'].mean().reset_index()\n",
    "\n",
    "# Sort the genres by mean rating in descending order\n",
    "sorted_genres = genre_ratings.sort_values(by='rating', ascending=False)\n",
    "\n",
    "# Get the top five highest-rated genres\n",
    "top_5_genres = sorted_genres.head(5)\n",
    "# Get the bottom five least-rated genres\n",
    "bottom_5_genres = sorted_genres.tail(5)\n",
    "# Combine the top and bottom genres\n",
    "combined_genres = pd.concat([top_5_genres, bottom_5_genres])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='genres', y='rating', data=combined_genres, hue='rating', palette='coolwarm', dodge=False)\n",
    "plt.title('Top and Bottom Genres by Mean Rating')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean Rating')\n",
    "plt.legend(title='Mean Rating', loc='upper right', labels=['Top 5', 'Bottom 5'])\n",
    "plt.xticks(rotation=45, ha='right')  # Adjust x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zSWJA9rzpB2"
   },
   "source": [
    "The above output displays the top 5 and bottom 5 genres based on their mean ratings. Genres like Animation|Drama|Sci-Fi|IMAX, and Adventure|Comedy|Fantasy|Musical have the highest ratings, both receiving a perfect 5.0. On the other hand, genres such as Action|Mystery|Sci-Fi|Thriller and Romance|War have lower ratings around 2.7. These ratings provide insights into the overall appreciation of different genres among viewers.\n",
    "Niche genres with low counts of movies in the datasets yet exhibiting high average rating such as Imax, Fantasy, and Animation may suggest a dedicated audience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeIzXnBr0TGN"
   },
   "source": [
    "#### Timestamp Analysis.\n",
    "\n",
    "##### How tagging and rating are distributed overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "W71xPmx70quO",
    "outputId": "52c61183-de51-4e2c-edd6-e62dbf4e1666"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp columns to datetime\n",
    "cleaned_df['tags_timestamp'] = pd.to_datetime(cleaned_df['tags_timestamp'], unit='s')\n",
    "cleaned_df['ratings_timestamp'] = pd.to_datetime(cleaned_df['ratings_timestamp'], unit='s')\n",
    "\n",
    "# Extract year from timestamps\n",
    "cleaned_df['tags_year'] = cleaned_df['tags_timestamp'].dt.year\n",
    "cleaned_df['ratings_year'] = cleaned_df['ratings_timestamp'].dt.year\n",
    "\n",
    "# Visualize the distribution of timestamps over the years\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(cleaned_df['tags_year'], label='Tags', kde=True)\n",
    "sns.histplot(cleaned_df['ratings_year'], label='Ratings', kde=True)\n",
    "plt.title('Distribution of Timestamps Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization allows us to observe how the number of tags and ratings has evolved over the years, providing insights into the temporal distribution of user interactions with movies.Flunctuations in the trend of ratings over the years may signal changes in user engagement.However, consistent rating over the years suggest that users continue to engage with the platform over an extended period.\n",
    "Tags seem to have originated around 2006, with an initial surge in count, reaching approximately 50,000. However, there is a subsequent decline in tag activity, followed by a resurgence starting from 2015. The counts continue to increase, reaching around 70,000 in 2017. This observation suggests a potential shift or evolution in user engagement with movies over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK2Yodi13hkB"
   },
   "source": [
    "#### Distribution of Ratings per user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Uci209y23A_B",
    "outputId": "9c31aa83-e472-4f36-bcb2-3de61791eb52"
   },
   "outputs": [],
   "source": [
    "# Top 10 most active users as per number of ratings.\n",
    "# Distribution of the number of ratings per user\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(cleaned_df['ratings_userId'].value_counts(), bins=50, kde=True, color='purple')\n",
    "plt.xlabel('Number of Ratings per User')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.show()\n",
    "\n",
    "# Identify and visualize the most active users\n",
    "top_users = cleaned_df['ratings_userId'].value_counts().head(10)\n",
    "\n",
    "# Plotting the number of ratings for the top users\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_users.plot(kind='bar', color='orange')\n",
    "plt.xlabel('User ID')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.title('Top 10 Most Active Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above shows us the power or active users i.e. the top ten users with highest count of ratings. The visualization offers useful insights that streaming companies can use for user segmentation and targeting. Higher ratings count may also indicate satisfied and retained customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij_ddUA25FvT"
   },
   "source": [
    " #### Movie popularity based on the number of ratings, average rating, and user engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBP1QCyr4Kf6",
    "outputId": "fb83ad0d-2643-43f8-a661-de3a84da6240",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of ratings per movie\n",
    "movie_ratings_count = cleaned_df.groupby('title')['rating'].count()\n",
    "\n",
    "# Calculate the average rating per movie\n",
    "movie_avg_rating = cleaned_df.groupby('title')['rating'].mean()\n",
    "\n",
    "# Calculate user engagement (sum of ratings for each movie)\n",
    "movie_user_engagement = cleaned_df.groupby('title')['ratings_userId'].sum()\n",
    "\n",
    "# Combine the calculated metrics into a DataFrame\n",
    "movie_popularity_df = pd.DataFrame({\n",
    "    'ratings_count': movie_ratings_count,\n",
    "    'avg_rating': movie_avg_rating,\n",
    "    'user_engagement': movie_user_engagement\n",
    "})\n",
    "\n",
    "# Identify the most popular movies based on ratings count\n",
    "top_movies_by_ratings = movie_popularity_df.sort_values(by='ratings_count', ascending=False).head(10)\n",
    "\n",
    "# Identify the most popular movies based on user engagement\n",
    "top_movies_by_engagement = movie_popularity_df.sort_values(by='user_engagement', ascending=False).head(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Movies by Ratings Count:\")\n",
    "print(top_movies_by_ratings)\n",
    "\n",
    "print(\"\\nTop 10 Movies by User Engagement:\")\n",
    "print(top_movies_by_engagement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pulp Fiction (1994) is the most rated movie with 54,119 ratings, having an average rating of 4.29.\n",
    "Fight Club (1999) follows with 11,610 ratings and an average rating of 4.33.\n",
    "The list includes other well-known movies like Star Wars: Episode IV - A New Hope (1977), Eternal Sunshine of the Spotless Mind (2004), and Inception (2010).\n",
    "\n",
    "Pulp Fiction (1994) maintains its top position in user engagement with a total of 16,197,509 ratings, consistent with its high ratings count.\n",
    "Fight Club (1999) and Star Wars: Episode IV - A New Hope (1977) are also prominent in user engagement.\n",
    "The user engagement metric considers the sum of ratings for each movie, providing a measure of the overall involvement of users with a particular movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0fKFS2a5ZNE"
   },
   "source": [
    "#### Genre Preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR14UfQD5i7q",
    "outputId": "074db021-55f2-49d1-9b99-586b787bb75b"
   },
   "outputs": [],
   "source": [
    "# Split the genres column into individual genres\n",
    "cleaned_df['genres'] = cleaned_df['genres'].str.split('|')\n",
    "\n",
    "# Create a DataFrame with exploded genres\n",
    "genres_df = cleaned_df.explode('genres')\n",
    "\n",
    "# Calculate the number of ratings per genre\n",
    "genre_ratings_count = genres_df.groupby('genres')['rating'].count()\n",
    "\n",
    "# Calculate the average rating per genre\n",
    "genre_avg_rating = genres_df.groupby('genres')['rating'].mean()\n",
    "\n",
    "# Calculate user engagement (sum of ratings for each genre)\n",
    "genre_user_engagement = genres_df.groupby('genres')['ratings_userId'].sum()\n",
    "\n",
    "# Combine the calculated metrics into a DataFrame\n",
    "genre_preferences_df = pd.DataFrame({\n",
    "    'ratings_count': genre_ratings_count,\n",
    "    'avg_rating': genre_avg_rating,\n",
    "    'user_engagement': genre_user_engagement\n",
    "})\n",
    "\n",
    "# Identify the most popular genres based on ratings count\n",
    "top_genres_by_ratings = genre_preferences_df.sort_values(by='ratings_count', ascending=False).head(10)\n",
    "\n",
    "# Identify the most popular genres based on user engagement\n",
    "top_genres_by_engagement = genre_preferences_df.sort_values(by='user_engagement', ascending=False).head(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Genres by Ratings Count:\")\n",
    "print(top_genres_by_ratings)\n",
    "\n",
    "print(\"\\nTop 10 Genres by User Engagement:\")\n",
    "print(top_genres_by_engagement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary highlight genres that are highly rated and also extensively engaged with by users in terms of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6jspcJ66Td8"
   },
   "source": [
    "#### User Retention overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "gf3U75FO6UYl",
    "outputId": "21d6bad5-aed5-4174-9bb4-4074946b9ec8"
   },
   "outputs": [],
   "source": [
    "# How often a user returns to the platform\n",
    "# Extract the year and month from timestamps\n",
    "cleaned_df['year_month'] = cleaned_df['ratings_timestamp'].dt.to_period('M')\n",
    "\n",
    "# Calculate the number of unique users each month\n",
    "monthly_unique_users = cleaned_df.groupby('year_month')['ratings_userId'].nunique()\n",
    "\n",
    "# Calculate user retention rate\n",
    "retention_rate = monthly_unique_users / monthly_unique_users.shift(1)\n",
    "\n",
    "# Plotting user retention over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=monthly_unique_users.index.astype(str), y=retention_rate, marker='o')\n",
    "#plt.xlabel('Year-Month')\n",
    "plt.xlabel('Year-Month', fontsize=12)  # Adjust font size\n",
    "plt.ylabel('Retention Rate', fontsize=12)  # Adjust font size\n",
    "\n",
    "#plt.ylabel('Retention Rate')\n",
    "plt.title('User Retention Over Time')\n",
    "# Specify the interval for x-axis ticks (e.g., every 3 months)\n",
    "plt.xticks(range(0, len(monthly_unique_users.index), 3), monthly_unique_users.index[::3], rotation=90)\n",
    "#plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user retention plot reveals distinctive patterns over time, suggesting fluctuations in the platform's ability to retain users. Key observations include a substantial peak around August to November 1999, followed by a pronounced decline in retention rates that stabilize with intermittent fluctuations until approximately August to November 2013. Subsequently, there is another dip, and the platform maintains a relatively stable retention rate with periodic increased peaks. These patterns may indicate critical periods of user engagement, challenges, or strategic changes that impacted user retention dynamics on the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyTZvxuo61yM"
   },
   "source": [
    "#### Popular Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify high-rated movies\n",
    "high_rated_movies = cleaned_df[cleaned_df['rating'] >= 4.0]\n",
    "\n",
    "# Identify popular movies based on the number of ratings\n",
    "popular_movies = cleaned_df.groupby('movieId')['rating'].count().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Filter high-rated movies based on popularity\n",
    "high_rated_popular_movies = high_rated_movies[high_rated_movies['movieId'].isin(popular_movies.index)]\n",
    "\n",
    "# Plotting high-rated and popular movies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=high_rated_popular_movies['title'], y=high_rated_popular_movies['rating'], palette='viridis', hue=high_rated_popular_movies['rating'], legend=False)\n",
    "plt.xlabel('Movie Title')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('High-Rated and Popular Movies (Top 10 by Ratings)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization showcases the average ratings of the top 10 high-rated and popular movies, combining both criteria. Notable movies like \"Inception,\" \"Forrest Gump,\" and \"Pulp Fiction\" emerge as popular choices with high average ratings, reinforcing their status as both well-received and frequently rated by users. Since they are highly rated and widely watched, they can make good candidates for recommending to other users with similar preferences enhancing the overall experience on the platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation calculation\n",
    "numeric_columns = cleaned_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = cleaned_df[numeric_columns].corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between the'userId' feature from the ratings file  and the 'ratings' is 0.026 while the correlation between 'userId' from the tags file and rating is 0.12 indicating how specific users tend to rate movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUivP9FN9l7f"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We engineer a column 'userId' which will be our pivot column for the user-item-matrix by combining the 'ratings_userId' and 'tags_userId columns'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'userId' with unique values based on 'movieId'\n",
    "cleaned_df['userId'] = cleaned_df[['ratings_userId', 'tags_userId']].apply(lambda row: row['ratings_userId'] if pd.notna(row['tags_userId']) else row['tages_userId'], axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cleaned_df = cleaned_df.drop(['ratings_userId', 'tags_userId'], axis=1)\n",
    "\n",
    "# Drop duplicates based on 'movieId' and 'userId'\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=['movieId', 'userId'])\n",
    "\n",
    "print(cleaned_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Collaborative filtering with k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering with k-NN will be our baseline model. It is a common and effective for recommendation systems. It's relatively simple to implement, interpretable, and can provide reasonable recommendations, especially when you have a moderate-sized dataset hence our choice for baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVuSucSD8eM1"
   },
   "source": [
    "#### User-Item Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix leads us to one of the advantages of collaborative filtering: it's excellent at discovering new and unexpected recommendations. Since it's based on user behavior, it can suggest a movie you might never have considered but will probably like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let’s create a user-movie rating matrix for our dataset\n",
    "user_item_matrix = cleaned_df.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user-movie rating matrix for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix can be sparse which could significantly increase the amount of computation resources needed. The code below checks for sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your matrix is a pandas DataFrame, you can convert user to a NumPy array\n",
    "user_item_matrix_np = user_item_matrix.values\n",
    "\n",
    "# Calculate sparsity\n",
    "total_elements = user_item_matrix.size\n",
    "non_zero_elements = np.count_nonzero(user_item_matrix_np)\n",
    "sparsity = (1 - non_zero_elements / total_elements) * 100\n",
    "\n",
    "print(f\"Sparsity of the user-movie matrix: {sparsity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity of the user-movie matrix is 95.02%. Sparsity is a measure of the proportion of zero or empty values in a matrix. In this context, it indicates that 95.02% of the entries in the user-movie matrix are empty or contain a zero value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the csr_matrix function to convert the dense user_movie_matrix into a more memory-efficient compressed sparse row (CSR) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a compressed sparse row matrix using csr_matrix\n",
    "user_item_csr_matrix = csr_matrix(user_item_matrix_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric parameter used here is Cosine similarity. It measures how similar two entities are irrespective of size.The code below defines a KNN model, the metric, and other parameters and is then fit on the user-item_csr_matrix created in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a KNN model on cosine similarity\n",
    "cf_knn_model= NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=10, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fitting the model on our matrix\n",
    "cf_knn_model.fit(user_item_csr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['movieId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user-defined function below check if the provided movieId is within the valid range for the rows of the user_item_csr_matrix.If 'valid' is returned true, then the movieId is passed through our recommender function which return a dataframe of the recommended movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_movieId(movieId, user_item_csr_matrix):\n",
    "    \"\"\"\n",
    "    Check if the provided movieId is within the valid range for the rows of the user-item matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - movieId: int, the movieId to check\n",
    "    - user_item_matrix: scipy.sparse.csr_matrix, the user-item matrix\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the movieId is within the valid range, False otherwise\n",
    "    \"\"\"\n",
    "    num_movies = user_item_csr_matrix.shape[0]\n",
    "    return 0 <= movieId < num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_id = is_valid_movieId(1, user_item_csr_matrix)\n",
    "valid_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function to provide the desired number of movie recommendations, given a movieId as input.\n",
    "The input parameters for the function are:\n",
    "\n",
    "*n_recs: The number of final recommendations that we would get as output\n",
    "\n",
    "*MovieId: Input movieId (a number), based on which we find new recommendations\n",
    "\n",
    "*Matrix: The user_item_csr_matrix\n",
    "\n",
    "*cf_model: cf_knn_model\n",
    "\n",
    "*data: cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommender_engine(matrix, cf_model, cleaned_df, movie_id, n_recs):\n",
    "    # Fit model on matrix\n",
    "    cf_knn_model.fit(user_item_csr_matrix)\n",
    "    \n",
    "    # Check if provided movieId is valid\n",
    "    if movie_id is not None and movie_id < matrix.shape[0]:\n",
    "        # Calculate neighbour distances\n",
    "        distances, indices = cf_knn_model.kneighbors(user_item_csr_matrix[movie_id], n_neighbors=n_recs)\n",
    "        movie_rec_ids = sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1]\n",
    "    \n",
    "        # List to store recommendations\n",
    "        cf_recs = []\n",
    "        for i in movie_rec_ids:\n",
    "            \n",
    "            #cf_recs.append({'Title': cleaned_df['title'][i[0]], 'Distance': i[1]})\n",
    "            cf_recs.append({'Title': cleaned_df['title'].iloc[i[0]], 'Distance': i[1]})\n",
    "\n",
    "    \n",
    "        # Select top number of recommendations needed\n",
    "        #df = pd.DataFrame(cf_recs, index=range(1, n_recs + 1))\n",
    "        df = pd.DataFrame(cf_recs, index=range(1, len(cf_recs) + 1))\n",
    "\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Invalid movieId: {movie_id}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Recommendations from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movie_recommender_engine(user_item_csr_matrix, cf_knn_model, cleaned_df, movie_id=1, n_recs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrix factorization using Funk Singular Value Decomposition (SVD) for personalized movie recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(cleaned_df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Implement Funk SVD\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.test(testset)\n",
    "accuracy = rmse(predictions)\n",
    "print(f'RMSE: {accuracy}')\n",
    "\n",
    "# Make personalized recommendations for a user\n",
    "def get_personalized_recommendations(user_id, n=10):\n",
    "    movies_seen_by_user = cleaned_df[cleaned_df['userId'] == user_id]['movieId'].tolist()\n",
    "    movies_to_predict = cleaned_df[~cleaned_df['movieId'].isin(movies_seen_by_user)]['movieId'].tolist()\n",
    "\n",
    "    predictions = [(movie_id, model.predict(user_id, movie_id).est) for movie_id in movies_to_predict]\n",
    "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    recommended_movie_ids = [movie_id for movie_id, _ in recommendations]\n",
    "    recommended_movies = cleaned_df[cleaned_df['movieId'].isin(recommended_movie_ids)][['movieId', 'title', 'genres']]\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "# Example: Get personalized recommendations for user with ID 1\n",
    "user_id_to_recommend = 1\n",
    "recommended_movies = get_personalized_recommendations(user_id_to_recommend)\n",
    "\n",
    "# Remove duplicate movie recommendations\n",
    "recommended_movies = recommended_movies.drop_duplicates(subset='movieId')\n",
    "print(recommended_movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD model achieves an rmse of approximately 74.93%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend movies to a user based on the preferences of other users who liked the same movies.The cosine matrix is used to find movies are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "# Assuming 'cleaned_df' is your DataFrame\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(cleaned_df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Build the item-item collaborative filtering model\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': False  # Item-item collaborative filtering\n",
    "}\n",
    "\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.test(testset)\n",
    "accuracy = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {accuracy}')\n",
    "\n",
    "# Make item-item collaborative filtering recommendations for a user\n",
    "def get_item_item_recommendations(user_id, n=10):\n",
    "    movies_seen_by_user = cleaned_df[cleaned_df['userId'] == user_id]['movieId'].tolist()\n",
    "    movies_to_predict = cleaned_df[~cleaned_df['movieId'].isin(movies_seen_by_user)]['movieId'].tolist()\n",
    "\n",
    "    predictions = [(movie_id, model.predict(user_id, movie_id).est) for movie_id in movies_to_predict]\n",
    "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    recommended_movie_ids = [movie_id for movie_id, _ in recommendations]\n",
    "    recommended_movies = cleaned_df[cleaned_df['movieId'].isin(recommended_movie_ids)][['movieId', 'title', 'genres']]\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "\n",
    "# Example: Get item-item collaborative filtering recommendations for user with ID 1\n",
    "user_id_to_recommend = 1\n",
    "item_item_recommendations = get_item_item_recommendations(user_id_to_recommend)\n",
    "\n",
    "# Remove duplicate movie recommendations\n",
    "item_item_recommendations = item_item_recommendations.drop_duplicates(subset='movieId')\n",
    "\n",
    "print(item_item_recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieves rmse of approximately 79.43%, indicating the accuracy of the item-item collaborative filtering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Collaborative Filtering (User-Item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Increase User Retention\n",
    "\n",
    " Recommend movies based on the preferences of similar users. Users who have liked similar movies will receive recommendations.\n",
    "\n",
    "Implementation: Use techniques like user-item collaborative filtering with algorithms such as Singular Value Decomposition (SVD) or Alternating Least Squares (ALS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(cleaned_df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Build the user-item collaborative filtering model using SVD\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.test(testset)\n",
    "accuracy = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {accuracy}')\n",
    "\n",
    "# Make user-item collaborative filtering recommendations for a user\n",
    "def get_user_item_recommendations(user_id, n=10):\n",
    "    movies_seen_by_user = cleaned_df[cleaned_df['userId'] == user_id]['movieId'].tolist()\n",
    "    movies_to_predict = cleaned_df[~cleaned_df['movieId'].isin(movies_seen_by_user)]['movieId'].tolist()\n",
    "\n",
    "    predictions = [(movie_id, model.predict(user_id, movie_id).est) for movie_id in movies_to_predict]\n",
    "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    recommended_movie_ids = [movie_id for movie_id, _ in recommendations]\n",
    "    recommended_movies = cleaned_df[cleaned_df['movieId'].isin(recommended_movie_ids)][['movieId', 'title', 'genres']]\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "\n",
    "# Example: Get user-item collaborative filtering recommendations for user with ID 1\n",
    "user_id_to_recommend = 1\n",
    "user_item_recommendations = get_user_item_recommendations(user_id_to_recommend)\n",
    "\n",
    "# Remove duplicate movie recommendations\n",
    "user_item_recommendations = user_item_recommendations.drop_duplicates(subset='movieId')\n",
    "\n",
    "print(user_item_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE has improved, and the output now contains a recommendation for the movie with 'movieId' 318, titled \"Shawshank Redemption, The (1994)\" in the genres of Crime and Drama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Neural Collaborative Filtering (NCF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = cleaned_df[['userId', 'movieId', 'rating']]\n",
    "\n",
    "# Encode user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "data['user'] = user_encoder.fit_transform(data['userId'])\n",
    "data['movie'] = movie_encoder.fit_transform(data['movieId'])\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the NCF model\n",
    "def create_ncf_model(num_users, num_movies, embedding_size=50, hidden_size=50):\n",
    "    # User embedding\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)\n",
    "    user_flat = Flatten()(user_embedding)\n",
    "\n",
    "    # Movie embedding\n",
    "    movie_input = Input(shape=(1,), name='movie_input')\n",
    "    movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size)(movie_input)\n",
    "    movie_flat = Flatten()(movie_embedding)\n",
    "\n",
    "    # Concatenate user and movie embeddings\n",
    "    concat = Concatenate()([user_flat, movie_flat])\n",
    "\n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(hidden_size, activation='relu')(concat)\n",
    "    output = Dense(1, activation='linear')(fc1)\n",
    "\n",
    "    model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Get the number of unique users and movies\n",
    "num_users = data['user'].nunique()\n",
    "num_movies = data['movie'].nunique()\n",
    "\n",
    "# Create the NCF model\n",
    "ncf_model = create_ncf_model(num_users, num_movies)\n",
    "\n",
    "# Train the model\n",
    "ncf_model.fit([train_data['user'], train_data['movie']], train_data['rating'], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = ncf_model.evaluate([test_data['user'], test_data['movie']], test_data['rating'])\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Make predictions\n",
    "user_id_to_predict = 1\n",
    "movie_id_to_predict = 318  # Movie ID for \"Shawshank Redemption, The (1994)\"\n",
    "user_input = np.array([user_encoder.transform([user_id_to_predict])[0]])\n",
    "movie_input = np.array([movie_encoder.transform([movie_id_to_predict])[0]])\n",
    "rating_prediction = ncf_model.predict([user_input, movie_input])\n",
    "print(f'Predicted Rating for User {user_id_to_predict} and Movie {movie_id_to_predict}: {rating_prediction[0][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training and Validation Loss:* The model was trained over 10 epochs, and you can observe the training and validation loss during each epoch. The loss decreases over epochs, indicating that the model is learning from the data.\n",
    "\n",
    "*Test Loss:* After training, the model was evaluated on the test set, and the test loss is approximately 0.5488. This value represents how well the model generalizes to new, unseen data.\n",
    "\n",
    "*Predicted Rating:* For a specific user (User 1) and movie (Movie with ID 318, \"Shawshank Redemption, The (1994)\"), the predicted rating is approximately 4.89. This is the model's estimate for how much the user might like the given movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Collaborative Filtering (NCF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    return 0.001 * 0.95 ** epoch\n",
    "\n",
    "# Create the NCF model with tuning\n",
    "def create_tuned_ncf_model(num_users, num_movies, embedding_size=50, hidden_size=50, dropout_rate=0.2):\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)\n",
    "    user_flat = Flatten()(user_embedding)\n",
    "\n",
    "    movie_input = Input(shape=(1,), name='movie_input')\n",
    "    movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size)(movie_input)\n",
    "    movie_flat = Flatten()(movie_embedding)\n",
    "\n",
    "    concat = Concatenate()([user_flat, movie_flat])\n",
    "    concat = Dropout(dropout_rate)(concat)\n",
    "\n",
    "    fc1 = Dense(hidden_size, activation='relu')(concat)\n",
    "    fc1 = Dropout(dropout_rate)(fc1)\n",
    "\n",
    "    output = Dense(1, activation='linear')(fc1)\n",
    "\n",
    "    model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the tuned NCF model\n",
    "tuned_ncf_model = create_tuned_ncf_model(num_users, num_movies, embedding_size=64, hidden_size=64, dropout_rate=0.4)\n",
    "\n",
    "# Train the model with the learning rate scheduler\n",
    "history = tuned_ncf_model.fit(\n",
    "    [train_data['user'], train_data['movie']],\n",
    "    train_data['rating'],\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[LearningRateScheduler(lr_schedule)]\n",
    ")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "test_loss = tuned_ncf_model.evaluate([test_data['user'], test_data['movie']], test_data['rating'])\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training and Validation Loss:* The model was trained over 20 epochs, and you can observe the training and validation loss during each epoch.\n",
    "\n",
    "*Test Loss:* After training, the model was evaluated on the test set, and the test loss is approximately 0.5508. This value represents how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training and validation loss from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create a plot for training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing the training loss and validation loss over epochs.\n",
    "\n",
    " The training loss decreases over epochs, indicating that the model is learning from the data.\n",
    "\n",
    " The training loss decreases but the validation loss  remains stagnant, it may indicate overfitting.\n",
    "\n",
    " The model has not yet stabilized , further training could lead to improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a higher dropout rate\n",
    "dropout_rate = 0.5  # Adjust as needed\n",
    "\n",
    "# Create the tuned NCF model with higher dropout\n",
    "tuned_ncf_model = create_tuned_ncf_model(num_users, num_movies, embedding_size=64, hidden_size=64, dropout_rate=dropout_rate)\n",
    "\n",
    "# Train the model with the learning rate scheduler\n",
    "history = tuned_ncf_model.fit(\n",
    "    [train_data['user'], train_data['movie']],\n",
    "    train_data['rating'],\n",
    "    epochs=20,  # adjust the number of epochs as needed\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[LearningRateScheduler(lr_schedule)]\n",
    ")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "test_loss = tuned_ncf_model.evaluate([test_data['user'], test_data['movie']], test_data['rating'])\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned model has been trained with increased dropout and adjusted hyperparameters. The test loss is evaluated to assess the model's performance.\n",
    "\n",
    "*Training Loss:* Gradually decreases over epochs, indicating that the model is learning from the training data.\n",
    "\n",
    "*Validation Loss:* Also decreases initially but starts to stabilize or slightly increase after a certain number of epochs. This might indicate that the model is overfitting or that further improvements are challenging.\n",
    "\n",
    "\n",
    "*Test Loss:* The test loss provides an indication of how well the model generalizes to unseen data. In this case, the test loss is 0.5536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training loss continues to decrease while the validation loss remains stagnant , it's a clear indication of overfitting.\n",
    "\n",
    " Overfitting occurs when the model learns the training data too well, including its noise and fluctuations, and fails to generalize well to new, unseen data.\n",
    "\n",
    "##### Here are some strategies  to address overfitting:\n",
    "\n",
    "*Increase Regularization:*\n",
    "\n",
    "Increase dropout rate: Further increase the dropout rate in layers to introduce more regularization.\n",
    "Add L1 or L2 regularization: Penalize large weights by adding L1 or L2 regularization terms to the loss function.\n",
    "\n",
    "*Reduce Model Complexity:*\n",
    "\n",
    "Decrease the number of hidden units: Reducing the complexity of the model can help prevent it from memorizing the training data.\n",
    "\n",
    "*Early Stopping:*\n",
    "\n",
    "Implement early stopping: Monitor the validation loss during training and stop the training process when the validation loss stops improving or starts to degrade.\n",
    "Data Augmentation:\n",
    "\n",
    "*If applicable,*\n",
    " use data augmentation techniques to artificially increase the diversity of your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the Dropout Rate and Implementing Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting the Dropout Rate and Implementing Early Stopping.\n",
    "\n",
    "# Define a higher dropout rate\n",
    "dropout_rate = 0.6  # Adjust as needed\n",
    "\n",
    "# Create the tuned NCF model with higher dropout\n",
    "tuned_ncf_model = create_tuned_ncf_model(num_users, num_movies, embedding_size=64, hidden_size=64, dropout_rate=dropout_rate)\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = tuned_ncf_model.fit(\n",
    "    [train_data['user'], train_data['movie']],\n",
    "    train_data['rating'],\n",
    "    epochs=50,  # You can adjust the number of epochs as needed\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[LearningRateScheduler(lr_schedule), early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "test_loss = tuned_ncf_model.evaluate([test_data['user'], test_data['movie']], test_data['rating'])\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjustments have improved the situation. The training history shows that the validation loss is decreasing, and the test loss is reasonable. The training process was stopped early (at epoch 50) based on the early stopping criteria.\n",
    "The test loss of approximately 0.54 indicates that the neural collaborative filtering model is performing reasonably well on unseen data.\n",
    "A lower test loss suggests that the model's predictions are closer to the actual user-item interactions in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(cleaned_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Instantiate the SVD model\n",
    "svd_model = SVD()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(svd_model, data, measures=['RMSE'], cv=5, verbose=True)\n",
    "\n",
    "# Get the average RMSE across folds\n",
    "average_rmse = cv_results['test_rmse'].mean()\n",
    "print(f'Average RMSE: {average_rmse * 100}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error): The RMSE on individual folds are as follows:\n",
    "\n",
    "Fold 1: 0.7337\n",
    "\n",
    "Fold 2: 0.7545\n",
    "\n",
    "Fold 3: 0.7273\n",
    "\n",
    "Fold 4: 0.7427\n",
    "\n",
    "Fold 5: 0.7462\n",
    "\n",
    "Mean RMSE: The mean RMSE across all folds is approximately 0.7408.\n",
    "\n",
    "Standard Deviation (Std) of RMSE: The standard deviation of RMSE across folds is 0.0095.\n",
    "\n",
    "Fit time: The average time taken for model fitting across folds is 0.88 seconds.\n",
    "\n",
    "Test time: The average time taken for predicting on the test set across folds is 0.08 seconds.\n",
    "\n",
    "In summary, the SVD algorithm has a reasonably low RMSE, indicating good predictive performance on the test set. The consistency of RMSE across folds is supported by a low standard deviation. The fit and test times are also within a reasonable range, suggesting efficiency in model training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation system has achieved a 74% accuracy in aligning user preferences with movie recommendations. This addresses the challenge of content navigation challenges by delivering tailored movie suggestions ultimately improving watching experience. An accurate recommendation system contributes significantly to user contentment and the platform's success. By streamlining movie searches and maximizing content enjoyment, personalized recommendations improve efficiency and foster loyalty. Users who consistently find appealing content are more likely to stay engaged and loyal, ensuring long-term user retention and platform prosperity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The streaming platform should aim to feature movies rated at least 3.5 and above as they cut across most users.\n",
    "\n",
    "2. There should be implementation of content-based recommendations which analyze movie attributes such as genre, actors, directors, and user preferences to make more diverse and personalized recommendations.\n",
    "\n",
    "3. Develop a hybrid recommender system which combine SVD model and content-based recommendation approach to capitalize on the strengths of both methods"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
